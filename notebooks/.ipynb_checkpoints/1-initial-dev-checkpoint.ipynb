{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for MobileNet development.\n",
    "\n",
    "## initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "def make_divisible(x, divisible_by=8):\n",
    "    import numpy as np\n",
    "    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        # input_channel = make_divisible(input_channel * width_mult)  # first channel is always 32!\n",
    "        self.last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = make_divisible(c * width_mult) if t > 1 else c\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Linear(self.last_channel, n_class)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "model = MobileNetV2(width_mult=1, n_class=10, input_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomPerspective(),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "dataset = torchvision.datasets.CIFAR10(\"cifar10/\", train=True, transform=transform, download=True)\n",
    "# dataset = torchvision.datasets.CIFAR10(\"/mnt/cifar10/\", train=True, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ex, y_ex = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4882e-02,  3.0258e-01, -5.8743e-01, -4.5475e-01, -2.8394e-02,\n",
       "         -5.1983e-02, -5.7043e-01,  2.4971e-01, -6.0351e-02,  8.4419e-03],\n",
       "        [ 6.0768e-02,  9.8650e-03,  2.4985e-01,  9.3584e-02, -1.2789e-01,\n",
       "         -2.3547e-01,  1.2566e-01, -1.2906e-01,  1.2965e-01, -1.2443e-01],\n",
       "        [-1.5583e-01,  2.7106e-01, -5.9658e-02,  5.3169e-02,  1.9452e-01,\n",
       "         -2.9189e-01, -4.0493e-01, -8.0180e-02,  2.1773e-01,  4.1286e-02],\n",
       "        [ 1.5030e-01, -1.9092e-01, -1.4071e-01, -3.7609e-01,  9.0936e-02,\n",
       "         -9.5844e-02, -3.9342e-01, -1.0372e-01, -2.7239e-01, -2.7863e-01],\n",
       "        [ 2.7413e-01,  2.7644e-01,  5.1805e-02,  1.5311e-01,  5.3775e-02,\n",
       "         -7.5813e-02, -2.9641e-01, -1.7854e-01,  1.3940e-01, -1.9341e-01],\n",
       "        [-6.1110e-02,  1.5751e-01, -5.9958e-02, -1.2210e-01,  2.0742e-01,\n",
       "          1.5072e-02, -2.0363e-01,  2.7187e-01,  1.4726e-01,  3.5018e-01],\n",
       "        [-2.0053e-01,  1.7566e-01, -4.2770e-01,  1.1256e-01, -1.7031e-01,\n",
       "          6.0348e-03, -4.8407e-01,  8.4267e-02, -7.1982e-02, -2.5840e-01],\n",
       "        [-9.9995e-02,  2.8331e-01, -2.6589e-02, -1.0875e-01, -1.2102e-01,\n",
       "         -2.8644e-01, -4.6784e-01,  1.0630e-01,  1.7541e-01, -3.1768e-02],\n",
       "        [-1.0837e-01,  5.4959e-01, -4.1486e-01,  7.4626e-01,  8.1028e-01,\n",
       "         -1.5365e-01, -4.9789e-01, -3.6078e-01,  3.8912e-01, -1.4025e-01],\n",
       "        [ 1.3113e-05,  2.0522e-01,  9.7103e-02,  5.0110e-02, -4.6666e-02,\n",
       "         -2.5201e-01, -3.5912e-01,  3.2953e-01,  4.2491e-01, -3.5362e-01],\n",
       "        [ 1.9146e-01,  2.5958e-01, -5.7977e-03,  1.4336e-01,  2.1285e-01,\n",
       "          1.9212e-01, -4.5416e-01, -6.6832e-02,  1.5102e-01, -3.7321e-01],\n",
       "        [-6.0819e-02,  3.3710e-01, -3.7717e-01,  1.9275e-01,  3.2750e-01,\n",
       "         -1.5425e-01, -3.4953e-01,  1.6324e-01, -8.8856e-02,  1.2095e-01],\n",
       "        [ 2.4977e-02,  2.8795e-01, -6.8819e-02,  1.9341e-01,  1.7892e-01,\n",
       "          2.1781e-02, -1.4765e-01, -4.4380e-01,  1.8903e-01, -4.5346e-01],\n",
       "        [-3.0997e-01, -2.3357e-02,  1.4917e-01,  2.9750e-01, -1.3511e-01,\n",
       "          1.1746e-01, -1.3846e-01,  1.1326e-01, -3.4898e-01,  4.3925e-01],\n",
       "        [ 7.0167e-02,  4.0073e-01, -8.4915e-02,  2.0390e-01,  9.5286e-02,\n",
       "          5.6564e-01,  6.6199e-03,  1.2957e-02,  3.7301e-01, -3.2237e-01],\n",
       "        [ 1.2832e-01,  2.5416e-01,  3.1361e-01, -1.8590e-01, -3.7858e-01,\n",
       "          1.8429e-01, -3.2009e-01, -1.6189e-01,  1.3027e-01,  9.2722e-02],\n",
       "        [-8.5556e-02,  3.0233e-01, -3.3954e-02,  1.1718e-01,  1.9356e-01,\n",
       "         -5.0549e-02, -3.9445e-01,  8.6338e-02,  1.1750e-01,  6.5315e-02],\n",
       "        [ 1.6181e-01, -2.8533e-01, -2.7328e-01,  5.3501e-01,  1.0803e-01,\n",
       "          5.1225e-02, -2.0309e-01, -2.5123e-02,  1.5147e-01, -2.0821e-01],\n",
       "        [-4.8245e-02, -1.7336e-01, -1.4092e-01, -7.7901e-03, -8.6542e-02,\n",
       "          3.2659e-02, -3.8868e-01, -8.6079e-02,  1.7410e-01, -2.5409e-01],\n",
       "        [-1.6254e-01,  2.9093e-01,  1.4186e-01,  2.2417e-01, -4.5830e-02,\n",
       "         -9.7297e-02, -2.4190e-01, -8.9762e-02,  1.5934e-01,  8.6453e-02],\n",
       "        [-6.9390e-02,  3.2907e-01,  1.7181e-01, -3.6767e-01,  2.6883e-01,\n",
       "         -2.8310e-01,  4.1828e-02,  1.3747e-01,  2.8649e-01, -6.8974e-02],\n",
       "        [-6.8319e-03,  3.7919e-01, -4.5225e-02,  2.1966e-01, -2.4331e-01,\n",
       "          5.1812e-02, -6.0804e-01, -9.6756e-02, -8.8972e-02,  7.2598e-02],\n",
       "        [ 6.7118e-02, -5.8385e-03, -1.5987e-02, -1.1703e-01,  2.1824e-02,\n",
       "         -6.6766e-02, -3.1098e-01,  2.3729e-01, -6.9210e-02, -2.7830e-01],\n",
       "        [ 1.3437e-01,  3.0081e-01,  7.0979e-02,  3.0859e-02,  2.6049e-01,\n",
       "         -2.5041e-01, -5.4716e-02, -8.2265e-02,  2.6881e-01, -1.5394e-01],\n",
       "        [-2.0835e-01,  2.8748e-02,  2.5773e-01,  4.2449e-01,  3.0792e-01,\n",
       "         -1.1840e-01, -2.2353e-01,  3.3091e-01,  1.5386e-01,  1.0340e-01],\n",
       "        [ 2.4759e-01,  2.0647e-01,  1.9255e-02, -1.5763e-01, -6.0295e-02,\n",
       "          5.0659e-02,  3.7733e-02,  1.8222e-02, -2.8461e-02, -1.3025e-01],\n",
       "        [ 3.7129e-01,  1.5894e-01, -4.7635e-01, -1.2448e-01, -2.6284e-01,\n",
       "         -1.5194e-01, -1.3998e-01,  2.4888e-01,  1.4370e-01, -3.5768e-01],\n",
       "        [ 7.3932e-02, -1.1026e-01,  3.0937e-01,  2.6476e-01, -1.2872e-01,\n",
       "         -2.0952e-02, -5.5445e-01,  6.2982e-01,  6.1995e-01, -1.8332e-01],\n",
       "        [-1.7506e-01,  9.1519e-02, -9.1638e-02,  3.1459e-01, -7.6850e-02,\n",
       "         -1.6734e-01, -2.9031e-01,  1.1880e-01,  4.5485e-01, -3.6728e-01],\n",
       "        [ 1.7602e-01,  2.7142e-01,  1.1148e-01,  8.1223e-02,  4.2488e-01,\n",
       "         -2.4385e-01, -1.4417e-02,  5.9411e-02, -2.2461e-01,  7.1762e-03],\n",
       "        [ 2.1645e-01,  3.7749e-01,  1.8967e-01, -2.6851e-02, -2.6119e-01,\n",
       "         -3.7703e-02,  1.2654e-01,  2.6476e-01,  8.6322e-02,  6.2596e-02],\n",
       "        [-3.2965e-01,  2.8808e-01, -2.7080e-01, -5.7979e-02,  3.8205e-02,\n",
       "         -1.1449e-01, -4.2804e-01,  1.7337e-02,  8.7701e-02,  1.0299e-01]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1/10, batch 0. Loss: 2.296.\n",
      "Finished epoch 1. avg loss: 2.296233892440796; median loss: 2.296233892440796\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def train():\n",
    "    NUM_EPOCHS = 10\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        losses = []\n",
    "\n",
    "        for i, (X_batch, y_cls) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y = y_cls\n",
    "            X_batch = X_batch\n",
    "            # y = y_cls.cuda()\n",
    "            # X_batch = X_batch.cuda()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_loss = loss.item()\n",
    "            if i % 200 == 0:\n",
    "                print(\n",
    "                    f'Finished epoch {epoch}/{NUM_EPOCHS}, batch {i}. Loss: {curr_loss:.3f}.'\n",
    "                )\n",
    "\n",
    "            losses.append(curr_loss)\n",
    "            break\n",
    "\n",
    "        print(\n",
    "            f'Finished epoch {epoch}. '\n",
    "            f'avg loss: {np.mean(losses)}; median loss: {np.min(losses)}'\n",
    "        )\n",
    "        break\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quantized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "A sequential container.\n",
       "Modules will be added to it in the order they are passed in the constructor.\n",
       "Alternatively, an ordered dict of modules can also be passed in.\n",
       "\n",
       "To make it easier to understand, here is a small example::\n",
       "\n",
       "    # Example of using Sequential\n",
       "    model = nn.Sequential(\n",
       "              nn.Conv2d(1,20,5),\n",
       "              nn.ReLU(),\n",
       "              nn.Conv2d(20,64,5),\n",
       "              nn.ReLU()\n",
       "            )\n",
       "\n",
       "    # Example of using Sequential with OrderedDict\n",
       "    model = nn.Sequential(OrderedDict([\n",
       "              ('conv1', nn.Conv2d(1,20,5)),\n",
       "              ('relu1', nn.ReLU()),\n",
       "              ('conv2', nn.Conv2d(20,64,5)),\n",
       "              ('relu2', nn.ReLU())\n",
       "            ]))\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/opt/miniconda3/envs/model-dev/lib/python3.7/site-packages/torch/nn/modules/container.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     ConvReLU2d, LinearReLU, ConvBn2d, ConvBnReLU2d, _DenseLayer, _Transition, ConvBNReLU, DeepLabHead, ASPPConv, ASPPPooling, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.Sequential?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forked from https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py\n",
    "import torch.nn as nn\n",
    "import torch.quantization\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "# NOTE(aleksey): assigning layers names makes them easier to reference in the fuse_module code later\n",
    "# on. fuse_modules takes a list of lists of layers as input, without named layers we'd have to use\n",
    "# something like ['features.0.1', 'features.0.2']. Needless to say, that's not exactly readable.\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('q', torch.quantization.QuantStub()),\n",
    "        ('conv2d', nn.Conv2d(inp, oup, 3, stride, 1, bias=False)),\n",
    "        ('batchnorm2d', nn.BatchNorm2d(oup)),\n",
    "        ('relu6', nn.ReLU6(inplace=True)),\n",
    "        ('dq', torch.quantization.DeQuantStub())\n",
    "    ]))\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('q', torch.quantization.QuantStub()),\n",
    "        ('conv2d', nn.Conv2d(inp, oup, 1, 1, 0, bias=False)),\n",
    "        ('batchnorm2d', nn.BatchNorm2d(oup)),\n",
    "        ('relu6', nn.ReLU6(inplace=True)),\n",
    "        ('dq', torch.quantization.DeQuantStub())\n",
    "    ]))\n",
    "\n",
    "def make_divisible(x, divisible_by=8):\n",
    "    import numpy as np\n",
    "    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(OrderedDict([\n",
    "                ('q', torch.quantization.QuantStub()),\n",
    "                # dw\n",
    "                ('conv2d_1', nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False)),\n",
    "                ('bnorm_2', nn.BatchNorm2d(hidden_dim)),\n",
    "                ('relu6_3', nn.ReLU6(inplace=True)),\n",
    "                # pw-linear\n",
    "                ('conv2d_4', nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False)),\n",
    "                ('bnorm_5', nn.BatchNorm2d(oup)),\n",
    "                ('dq', torch.quantization.DeQuantStub())\n",
    "            ]))\n",
    "        else:\n",
    "            self.conv = nn.Sequential(OrderedDict([\n",
    "                ('q', torch.quantization.QuantStub()),\n",
    "                # pw\n",
    "                ('conv2d_1', nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False)),\n",
    "                ('bnorm_2', nn.BatchNorm2d(hidden_dim)),\n",
    "                ('relu6_3', nn.ReLU6(inplace=True)),\n",
    "                # dw\n",
    "                ('conv2d_4', nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False)),\n",
    "                ('bnorm_5', nn.BatchNorm2d(hidden_dim)),\n",
    "                ('relu6_6', nn.ReLU6(inplace=True)),\n",
    "                # pw-linear\n",
    "                ('conv2d_7', nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False)),\n",
    "                ('bnorm_8', nn.BatchNorm2d(oup)),\n",
    "                ('dq', torch.quantization.DeQuantStub())\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        # input_channel = make_divisible(input_channel * width_mult)  # first channel is always 32!\n",
    "        self.last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = make_divisible(c * width_mult) if t > 1 else c\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        submodule_names = [\n",
    "            'in_conv',\n",
    "            *[f'inv_conv_{i}' for i in range(1, 18)],\n",
    "            'out_conv'\n",
    "        ]\n",
    "        self.features = nn.Sequential(OrderedDict(list(zip(submodule_names, self.features))))\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Linear(self.last_channel, n_class)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "model = MobileNetV2(width_mult=1, n_class=10, input_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (in_conv): Sequential(\n",
       "      (q): QuantStub()\n",
       "      (conv2d): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batchnorm2d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu6): ReLU6(inplace=True)\n",
       "      (dq): DeQuantStub()\n",
       "    )\n",
       "    (inv_conv_1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bnorm_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (inv_conv_17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (q): QuantStub()\n",
       "        (conv2d_1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_3): ReLU6(inplace=True)\n",
       "        (conv2d_4): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bnorm_5): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu6_6): ReLU6(inplace=True)\n",
       "        (conv2d_7): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bnorm_8): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (dq): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (out_conv): Sequential(\n",
       "      (q): QuantStub()\n",
       "      (conv2d): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm2d): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu6): ReLU6(inplace=True)\n",
       "      (dq): DeQuantStub()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=1280, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import numpy as np\n",
    "import torch.quantization\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def eval_fn(model):\n",
    "    model.eval()\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "    \n",
    "    # TODO: module fusion would go here\n",
    "    model = torch.quantization.prepare(model)\n",
    "    model = torch.quantization.fuse_modules(\n",
    "        model,\n",
    "        [\n",
    "            # NOTE(aleksey): 'features' is the attr containing the non-head layers.\n",
    "            ['features.in_conv.conv2d', 'features.in_conv.batchnorm2d'],\n",
    "            ['features.inv_conv_1.conv.conv2d_1', 'features.inv_conv_1.conv.bnorm_2'],\n",
    "            ['features.inv_conv_1.conv.conv2d_4', 'features.inv_conv_1.conv.bnorm_5'],\n",
    "            *[\n",
    "                *[[f'features.inv_conv_{i}.conv.conv2d_1',\n",
    "                   f'features.inv_conv_{i}.conv.bnorm_2'] for i in range(2, 18)],\n",
    "                *[[f'features.inv_conv_{i}.conv.conv2d_4',\n",
    "                   f'features.inv_conv_{i}.conv.bnorm_5'] for i in range(2, 18)],\n",
    "                *[[f'features.inv_conv_{i}.conv.conv2d_7',\n",
    "                   f'features.inv_conv_{i}.conv.bnorm_8'] for i in range(2, 18)]\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    for i, (X_batch, y_cls) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y = y_cls\n",
    "        X_batch = X_batch\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        break\n",
    "\n",
    "    model = torch.quantization.convert(model)\n",
    "\n",
    "eval_fn(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finished scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../models/model_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../models/model_1.py\n",
    "\"\"\"\n",
    "Initial MobileNet model. Trained on CIFAR10.\n",
    "\"\"\"\n",
    "\n",
    "# Forked from https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py\n",
    "\n",
    "####################\n",
    "# MODEL DEFINITION #\n",
    "####################\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "def make_divisible(x, divisible_by=8):\n",
    "    import numpy as np\n",
    "    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        # input_channel = make_divisible(input_channel * width_mult)  # first channel is always 32!\n",
    "        self.last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = make_divisible(c * width_mult) if t > 1 else c\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Linear(self.last_channel, n_class)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "model = MobileNetV2(width_mult=1, n_class=10, input_size=32)\n",
    "\n",
    "\n",
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomPerspective(),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "dataset = torchvision.datasets.CIFAR10(\"/mnt/cifar10/\", train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "if torch.cuda.device_count() >= 1:\n",
    "    device = torch.device('cuda')\n",
    "    is_cpu = False\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    is_cpu = True\n",
    "\n",
    "def train(model):\n",
    "    model.to(device)\n",
    "    \n",
    "    NUM_EPOCHS = 10\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        losses = []\n",
    "\n",
    "        for i, (X_batch, y_cls) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if is_cpu:\n",
    "                y = y_cls\n",
    "                X_batch = X_batch\n",
    "            else:\n",
    "                y = y_cls.cuda()\n",
    "                X_batch = X_batch.cuda()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_loss = loss.item()\n",
    "            if i % 200 == 0:\n",
    "                print(\n",
    "                    f'Finished epoch {epoch}/{NUM_EPOCHS}, batch {i}. Loss: {curr_loss:.3f}.'\n",
    "                )\n",
    "\n",
    "            losses.append(curr_loss)\n",
    "\n",
    "        print(\n",
    "            f'Finished epoch {epoch}. '\n",
    "            f'avg loss: {np.mean(losses)}; median loss: {np.min(losses)}'\n",
    "        )\n",
    "        \n",
    "        if not os.path.exists(\"/spell/checkpoints/\"):\n",
    "            os.mkdir(\"/spell/checkpoints\")\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save(model.state_dict(), \"/spell/checkpoints/model_{epoch}.pth\")\n",
    "\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "spell run --github-url https://github.com/ResidentMario/mobilenet-cifar10.git \\\n",
    "  --machine-type t4 --github-ref dev \\\n",
    "  python models/model_1.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../servers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../servers/eval_quantized.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../servers/eval_quantized.py\n",
    "\"\"\"\n",
    "MobileNet model with quantization-aware training (QAT) enabled. Trained on CIFAR10.\n",
    "\n",
    "This file batches training and evaluation into the same script. Since QAT requires careful\n",
    "management of the training loop, it's easiest do both in the same run.\n",
    "\"\"\"\n",
    "# Forked from https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py\n",
    "\n",
    "# TODO: can we train on CUDA, then quantize on CPU? This bears investigating.\n",
    "\n",
    "####################\n",
    "# MODEL DEFINITION #\n",
    "####################\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.quantization\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "\n",
    "# NOTE(aleksey): assigning layers names makes them easier to reference in the fuse_module code later\n",
    "# on. fuse_modules takes a list of lists of layers as input, without named layers we'd have to use\n",
    "# something like ['features.0.1', 'features.0.2']. Needless to say, that's not exactly readable.\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('q', torch.quantization.QuantStub()),\n",
    "        ('conv2d', nn.Conv2d(inp, oup, 3, stride, 1, bias=False)),\n",
    "        ('batchnorm2d', nn.BatchNorm2d(oup)),\n",
    "        ('relu6', nn.ReLU6(inplace=True)),\n",
    "        ('dq', torch.quantization.DeQuantStub())\n",
    "    ]))\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('q', torch.quantization.QuantStub()),\n",
    "        ('conv2d', nn.Conv2d(inp, oup, 1, 1, 0, bias=False)),\n",
    "        ('batchnorm2d', nn.BatchNorm2d(oup)),\n",
    "        ('relu6', nn.ReLU6(inplace=True)),\n",
    "        ('dq', torch.quantization.DeQuantStub())\n",
    "    ]))\n",
    "\n",
    "def make_divisible(x, divisible_by=8):\n",
    "    import numpy as np\n",
    "    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(OrderedDict([\n",
    "                ('q', torch.quantization.QuantStub()),\n",
    "                # dw\n",
    "                ('conv2d_1', nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False)),\n",
    "                ('bnorm_2', nn.BatchNorm2d(hidden_dim)),\n",
    "                ('relu6_3', nn.ReLU6(inplace=True)),\n",
    "                # pw-linear\n",
    "                ('conv2d_4', nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False)),\n",
    "                ('bnorm_5', nn.BatchNorm2d(oup)),\n",
    "                ('dq', torch.quantization.DeQuantStub())\n",
    "            ]))\n",
    "        else:\n",
    "            self.conv = nn.Sequential(OrderedDict([\n",
    "                ('q', torch.quantization.QuantStub()),\n",
    "                # pw\n",
    "                ('conv2d_1', nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False)),\n",
    "                ('bnorm_2', nn.BatchNorm2d(hidden_dim)),\n",
    "                ('relu6_3', nn.ReLU6(inplace=True)),\n",
    "                # dw\n",
    "                ('conv2d_4', nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False)),\n",
    "                ('bnorm_5', nn.BatchNorm2d(hidden_dim)),\n",
    "                ('relu6_6', nn.ReLU6(inplace=True)),\n",
    "                # pw-linear\n",
    "                ('conv2d_7', nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False)),\n",
    "                ('bnorm_8', nn.BatchNorm2d(oup)),\n",
    "                ('dq', torch.quantization.DeQuantStub())\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        # input_channel = make_divisible(input_channel * width_mult)  # first channel is always 32!\n",
    "        self.last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = make_divisible(c * width_mult) if t > 1 else c\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        submodule_names = [\n",
    "            'in_conv',\n",
    "            *[f'inv_conv_{i}' for i in range(1, 18)],\n",
    "            'out_conv'\n",
    "        ]\n",
    "        self.features = nn.Sequential(OrderedDict(list(zip(submodule_names, self.features))))\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Linear(self.last_channel, n_class)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = MobileNetV2(width_mult=1, n_class=10, input_size=32)\n",
    "\n",
    "\n",
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomPerspective(),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "dataset = torchvision.datasets.CIFAR10(\"/mnt/cifar10/\", train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "def prepare_model(model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "    model = torch.quantization.fuse_modules(\n",
    "        model,\n",
    "        [\n",
    "            # NOTE(aleksey): 'features' is the attr containing the non-head layers.\n",
    "            ['features.in_conv.conv2d', 'features.in_conv.batchnorm2d'],\n",
    "            ['features.inv_conv_1.conv.conv2d_1', 'features.inv_conv_1.conv.bnorm_2'],\n",
    "            ['features.inv_conv_1.conv.conv2d_4', 'features.inv_conv_1.conv.bnorm_5'],\n",
    "            *[\n",
    "                *[[f'features.inv_conv_{i}.conv.conv2d_1',\n",
    "                   f'features.inv_conv_{i}.conv.bnorm_2'] for i in range(2, 18)],\n",
    "                *[[f'features.inv_conv_{i}.conv.conv2d_4',\n",
    "                   f'features.inv_conv_{i}.conv.bnorm_5'] for i in range(2, 18)],\n",
    "                *[[f'features.inv_conv_{i}.conv.conv2d_7',\n",
    "                   f'features.inv_conv_{i}.conv.bnorm_8'] for i in range(2, 18)]\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "    model = torch.quantization.prepare_qat(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    print(f\"Training the model...\")\n",
    "    start_time = time.time()\n",
    "    NUM_EPOCHS = 10\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        losses = []\n",
    "\n",
    "        for i, (X_batch, y_cls) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y = y_cls\n",
    "            X_batch = X_batch\n",
    "            # y = y_cls.cuda()\n",
    "            # X_batch = X_batch.cuda()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_loss = loss.item()\n",
    "            if i % 200 == 0:\n",
    "                print(\n",
    "                    f'Finished epoch {epoch}/{NUM_EPOCHS}, batch {i}. Loss: {curr_loss:.3f}.'\n",
    "                )\n",
    "\n",
    "            losses.append(curr_loss)\n",
    "\n",
    "        print(\n",
    "            f'Finished epoch {epoch}. '\n",
    "            f'avg loss: {np.mean(losses)}; median loss: {np.min(losses)}'\n",
    "        )\n",
    "    print(f\"Training done in {str(time.time() - start_time)} seconds.\")\n",
    "\n",
    "\n",
    "def eval_fn(model):\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Quantizing the model (post-training)...\")\n",
    "    start_time = time.time()\n",
    "    model = torch.quantization.convert(model)\n",
    "    print(f\"Quantization done in {str(time.time() - start_time)} seconds.\")\n",
    "\n",
    "    print(f\"Evaluating the model...\")\n",
    "    start_time = time.time()\n",
    "    for i, (X_batch, y_cls) in enumerate(dataloader):\n",
    "        y = y_cls\n",
    "        y_pred = model(X_batch)\n",
    "    print(f\"Evaluation done in {str(time.time() - start_time)} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = get_model()\n",
    "    prepare_model(model)\n",
    "    train(model)\n",
    "    eval_fn(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
